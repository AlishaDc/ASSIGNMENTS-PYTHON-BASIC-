{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed428b7d",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "\n",
    "Dependent variable is one whose values are dependent on the values of independent variable. In other words, the independent variable's values dictate the values of the dependent variable. In a linear equation, this would mean that the dependent variable would be in a linear relationship with the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb07aa",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "\n",
    "The main concept is to make a line fit data points of highly correlated pair of variables such that the root mean square error value is the least. This would mean that we need to make sure that the distance between the fitted regression line and the data points(residuals) is least. Several assumptions are made to make simple linear regression work. Linearity, Independence of variables involved, residuals must be normally distributed are assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6092d",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "The numerical property of a line that expresses the direction of the line and steepness of the line is called the slope. Slope can be calculated as\n",
    "\n",
    "m=(y2-y1)/(x2-x1)\n",
    "\n",
    "Slope is important in linear regression as it establishes the relationship(negatively linear or positively linear) between feature and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f575b5",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "\n",
    "Slope would be 3/2 or 1.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9439bd",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "\n",
    "For a positive slope, the two linearly related variables must have a positive correlation coefficient. (positive slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c389d0",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "\n",
    "For a positive slope, the two linearly related variables must have a negative correlation coefficient. (negative slope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2fde3",
   "metadata": {},
   "source": [
    "# Q7.\n",
    "\n",
    "Multiple linear regression is an extension of the concept of simple linear regression where a single dependent variable is linearly correlated and is in linear relationship with multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0336c3",
   "metadata": {},
   "source": [
    "# Q8.\n",
    "Sum of squares (SS) is a statistical tool that is used to identify the dispersion of data as well as how well the data can fit the model in regression analysis. The sum of squares got its name because it is calculated by finding the sum of the squared differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019177c5",
   "metadata": {},
   "source": [
    "# Q9.\n",
    "\n",
    "Regression sum of squares (also known as the sum of squares due to regression or explained sum of squares) The regression sum of squares describes how well a regression model represents the modeled data. A higher regression sum of squares indicates that the model does not fit the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa1daf",
   "metadata": {},
   "source": [
    "# Q10.\n",
    "\n",
    "In regression, \"multicollinearity\" refers to predictors that are correlated with other predictors. Multicollinearity occurs when your model includes multiple factors that are correlated not just to your response variable, but also to each other. In other words, it results when you have factors that are a bit redundant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256503a",
   "metadata": {},
   "source": [
    "# Q11.\n",
    "\n",
    "Heteroskedasticity (or heteroscedasticity) happens when the standard deviations of a predicted variable, monitored over different values of an independent variable or as related to prior time periods, are non-constant. With heteroskedasticity, the tell-tale sign upon visual inspection of the residual errors is that they will tend to fan out over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6525c3c3",
   "metadata": {},
   "source": [
    "# Q12.\n",
    "Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc411127",
   "metadata": {},
   "source": [
    "# Q13.\n",
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2380d09",
   "metadata": {},
   "source": [
    "# Q14.\n",
    "Polynomial regression models can bend. They can be constructed to the nth-degree to minimize squared error and maximize rsquared. Depending on the nth degree, the line of best fit can have more or less curves. The higher the exponent, the more numerous the curves. y=m0+m1x^1+m2x^2 Here y:dependent variable m0 is the constant mi is the slope and x^i is the dependent variable and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5597bcd",
   "metadata": {},
   "source": [
    "# Q15.\n",
    "A basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.This is a generalization of linear regression that essentially replaces each input with a function of the input. (A linear basis function model that uses the identity function is just linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b242f",
   "metadata": {},
   "source": [
    "# Q16.\n",
    "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative distribution function of logistic distribution. log(p/1-p) is the link function. Logarithmic transformation on the outcome variable allows us to model a non-linear association in a linear way. This is the equation used in Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
