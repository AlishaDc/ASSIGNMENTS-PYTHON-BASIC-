{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb24fc64",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "\n",
    "Support Vector Machine(SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well. Support Vectors are simply the co-ordinates of individual observation. The SVM classifier is a frontier which best segregates the two classes (hyper-plane/ line)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34627ff",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac368ff5",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "Because Support Vector Machine (SVM) optimization occurs by minimizing the decision vector w, the optimal hyperplane is influenced by the scale of the input features and it’s therefore recommended that data be standardized (mean 0, var 1) prior to SVM model training. Advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. Another advantage is to avoid numerical difficulties during the kernel calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962fddb",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "\n",
    "An SVM classifier can give the distance between the test instance and the decision boundary as output, so we can use that as a confidence score, but we cannot use this score to directly converted it into class probabilities. But if you set probability=True when building a model of SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM’s scores. By using this techniques, we can add the predict_proba() and predict_log_proba() methods to the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee7ede",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "\n",
    "This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is proportional to a number between m2 and m3. So if there are millions of instances, you should definitely use the primal form, because the dual form will be much too slow. The dual problem is faster to solve than the primal when the number of training instances is smaller than the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88877f1",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "\n",
    "There might be too much regularization. To decrease it, you need to increase gamma or C (or both). Increasing gamma makes the bell-shape curve narrower, and as a result each instance's range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances. Conversely, a small gamma value makes the bell-shaped curve wider, so instances have a larger range of influence, and the decision boundary ends up smoother. A smaller C value leads to a wider street but more margin violations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76fa4d",
   "metadata": {},
   "source": [
    "# Q8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da07f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris['data'][:, (2, 3)] # petal length, petal width\n",
    "y = iris['target']\n",
    "\n",
    "#Since Target Variable also has Virginica in it so we’re just taking \n",
    "#out indexes which have 0 or 1 (you can print y and check for \n",
    "#yourself just write print(y))\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "\n",
    "X = X[setosa_or_versicolor]\n",
    "\n",
    "y = y[setosa_or_versicolor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb855fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#LinearSVC classifier\n",
    "lin_clf = LinearSVC(loss='hinge', C=5, random_state=42)\n",
    "\n",
    "#SVM classifier \n",
    "svm_clf = SVC(kernel='linear', C=5)\n",
    "\n",
    "#SGDClassifier \n",
    "sgd_clf = SGDClassifier(loss='hinge', learning_rate='constant', eta0=0.001, max_iter=1000, tol=1e-3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a64531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=5, loss='hinge', random_state=42)\n",
      "SVC(C=5, kernel='linear')\n",
      "SGDClassifier(eta0=0.001, learning_rate='constant', random_state=42)\n"
     ]
    }
   ],
   "source": [
    "print(lin_clf.fit(X, y))\n",
    "\n",
    "print(svm_clf.fit(X, y))\n",
    "\n",
    "print(sgd_clf.fit(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb5c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC [-3.17390003] [[0.869491   1.30457973]]\n",
      "SVC [-3.78823471] [[1.29411744 0.82352928]]\n",
      "SGDClassifier(alpha=0.00010): [-2.064] [[0.55126758 0.86023639]]\n"
     ]
    }
   ],
   "source": [
    "print('Linear SVC', lin_clf.intercept_, lin_clf.coef_)\n",
    "\n",
    "print('SVC', svm_clf.intercept_, svm_clf.coef_)\n",
    "\n",
    "print('SGDClassifier(alpha={:.5f}):'.format(sgd_clf.alpha), sgd_clf.intercept_, sgd_clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044fdea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
