{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe355522",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "\n",
    "Any tangible, measurable data in form of a list, or column that becomes the independent variable for any machine learning modelling problem, is called a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "847f1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = [4,5,3,4,3,7,9,10,3,2,4] #An example for a randomly sampled experience feature\n",
    "salary = [25000,30000,20000,25000,20000,60000,80000,100000,20000,15000,25000] #target variable values for each independent var\n",
    "#From above feature exp, linear regression can be trained to predict the target variable salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a9081",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "\n",
    "\n",
    "1. When more accurate model has to be created from limited data at hand.\n",
    "2. When improvement in generalizability is required.\n",
    "3. When improvement in model interpretability is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a0fdf",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "\n",
    "Nominal variables can be encoded in several ways\n",
    "\n",
    "1. One Hot Encoding Binary columns equal to the number of class labels present in the nominal variable are created in the form of sparse matrix, such that 0 indicates absence of a class label in a binary column and 1 indicates presence of a class label. Large number of such new variables or dummy variables can lead to the problem of dummy variable trap.\n",
    "\n",
    "2. Integer Encoding Values of a nominal variable can be changed into integer values. A column containing class labels New Delhi and Mumbai can be encoded into integer values in the form of 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80956be",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "\n",
    "Numeric features or ordinal features can be converted into categorical features by putting values of a numerical feature into bins or intervals. These intervals then become class labels and the feature becomes a categorical feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432a7b2",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "\n",
    "Wrapper approach is an iterative approach in feature selection where all combinations of subset of features is used to train the model and a perfomance metric is used to test the model and check of there is an improvement of score over the previous iteration where a different subset of features were used. Examples of this technique are forward feature selection and backward feature selection.\n",
    "\n",
    "Advantage is that it works better than filter approach. Disadvantage is that it requires a lot of time to compute feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da74774",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "\n",
    "A feature is considered irrelevant when it does not contribute or is detrimental to the performance of a model. Feature selection methods can be used to quantify a feature's relevance. For example, in a Multiple Linear Regression problem, correlation coefficient can be used to quantify relevance of a feature.\n",
    "\n",
    "Techniques like forward feature selection can be used to iteratively quantify which features do not contribute to the accuracy of a machine learning model and can be removed.\n",
    "\n",
    "Feature importance is supported by many machine learning models like Decision Trees and it can be used to quantify relevance of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf69743",
   "metadata": {},
   "source": [
    "# Q7.\n",
    "A feature is considered redundant when it does not contribute to the performance of a model by providing unique information.\n",
    "\n",
    "Criteria like Multicollinearity, non-unique information contribution, low feature importance score, little to no improvement in performance during forward and backward feature selection techniques, ANOVA etc help identify features that could be redundant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284ec31",
   "metadata": {},
   "source": [
    "# Q8.\n",
    "\n",
    "# 1. Euclidean(L2) Distance\n",
    "The most commonly used measure of distance, it is the length of shortest line between any two points x1 and x2. For any 2 dimensional data, we can write the formula as d = ((p1-q1)^2+(p2-q2)^2)^1/2\n",
    "\n",
    "# 2. Minkowski Distance\n",
    "It is a generalization of both Euclidean and Manhattan Distance. d=(sigma(|pi-qi|^p))^1/p where p is the value of order of normalization. When p=1, it represents Manhattan Distance and when p=2, it represents Euclidean Distance.\n",
    "\n",
    "# 3. Hamming Distance\n",
    "It is measured as the number of locations at which binary vectors or binary values from two features differ. Hamming distance becomes very important while handling data of gene sequencing and gene code.\n",
    "\n",
    "# 4. Manhattan(L1) Distance\n",
    "Sum of absolute differences between points across all dimensions. It can be written as d=sigma(|pi-qi|) where pi and qi are values for i to n data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac08e83",
   "metadata": {},
   "source": [
    "# Q9.\n",
    "\n",
    "Euclidean distance is the length of the shortest line between any two points x1 and x2. Manhattan distance is the sum of absolute differences between points across all dimensions.\n",
    "\n",
    "Euclidean distance is mathematically written d = ((p1-q1)^2+(p2-q2)^2)^1/2. Manhattan distance is d=sigma(|pi-qi|) where pi and qi are values for i to n data points.\n",
    "\n",
    "Euclidean Distance performs poorly on high dimensional data. Manhattan distance works better than Euclidean Distance for high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6452f",
   "metadata": {},
   "source": [
    "# Q10.\n",
    "\n",
    "Transforming datapoints into higher or lower dimension, or simply into features that can be used better for training phase and creating new features from existing features is called Feature Engineering. Feature Selection is a method used in which techniques can be used to select features based on their contribution to the model both information and performance-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29886ac5",
   "metadata": {},
   "source": [
    "# Q11.\n",
    "\n",
    "1. SVD (Standard Variable Diameter Diameter)\n",
    "2. Collection of features using a hybrid approach\n",
    "3. The width of the silhouette\n",
    "4. Receiver operating characteristic curve\n",
    " # iii. The width of the silhouette\n",
    "Estimate of average inter cluster distance to give efficacy/performance of cluster algorithms is called width of the silhouette. It can also be defined as how identical/similar a data point 'x' is to the data points inside the cluster to which x is assigned. Its value ranges from -1 to 1 where 1 means good and -1 means bad.\n",
    "\n",
    "# iv. Receiver operating characteristic curve\n",
    "Curve plotted between True Positive Rate and False Positive Rate is Receiver Operating Characteristics curve and is used to find the area under the curve for ROC-AUC score for binary classification evaluation. True Positive Rate is calculated as Number of positives that were correctly classified divided by total number of positives. False Positive Rate is calculated as the number of positives incorrectly classified divided by total number of negatives.\n",
    "\n",
    "True Positive Rate and False Positive Rate are calculated for different thresholds values where thresholds take values starting from the highest probability scores assigned to data points and goes up to the lowest probability score.\n",
    "\n",
    "The curve is impacted by presence of outliers, and simple models. Extensions can be made to this curve to suit multiclass classification evaluation requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524e769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
