{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb03a6e4",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "\n",
    "\n",
    "Using domain knowledge of data to transform features into higher or lower dimension and to create new features from existing features is feature engineering.\n",
    "\n",
    "Various aspects of feature engineering include:\n",
    "\n",
    "1. Understanding the problem statement.\n",
    "2. Train a baseline model and calculate its performance metric\n",
    "3. Reaching a conclusion about what features to create\n",
    "4. Create features\n",
    "5. Calculating how good new features contribute by training and testing and comparing the baseline performance metric\n",
    "6. Improve the features if need be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3834ec",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "\n",
    "Feature selection is the process of calculating the contribution in terms of information and performance increase to the machine learning model and removing those features that have little to no contribution or are detrimental to the model. Feature selection works by finding irrelevant and redundant features and removing them from the machine learning process.\n",
    "\n",
    "Aim of feature selection is to decrease learning time, performance and increase model interpretability.\n",
    "\n",
    "1. Filter methods Computationally less taxing, filter methods rely on finding the statistics for each feature. Example: Correlation coefficient.\n",
    "2. Wrapper methods All possibilities of subsets from features are tested by training a machine learning model on it and weighing the subset's quality using performance metrics. They are computationally taxing but have greater accuracy. Example: Forward Feature Selection\n",
    "3. Embedded methods At the expense of manageable computational costs, Embedded methods provide the benefits of both Filter and Wrapper methods.\n",
    "4. Hybrid methods Methods like feature ranking are used in Hybrid methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1f9c3",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "\n",
    "Filter methods rely on finding the statistics for each feature to select the features with highest contributions to the machine learning model. These statistics are calculated for both categorical and numerical data. For numerical data, statistics like Correlation coefficients and for categorical data, statistics like the Chi-Square test are applied between two features to find probability of correlation(linear and non-linear).\n",
    "\n",
    "Advantage:--- Less computational cost Disadvantage:--- Less accurate, cannot handle multicollinearity\n",
    "\n",
    "Wrapper methods rely on an iterative approach where model is trained over subsets of features and tested using a performance metric, using which we can decide to keep features or discard them by comparing the recent performance score with the last score.\n",
    "\n",
    "Advantage:--- Most accurate Disadvantage:--- Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75e2a3",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "\n",
    "# i. Describe the overall feature selection process.\n",
    "Using the problem statement, a feature selection approach is selected keeping in mind the advantages and disadvantages that come with it. Data is cleaned thoroughly.\n",
    "\n",
    "Next, we choose a method of feature selection technique from the feature selection approach. Features are selected or included into the main model training based on univariate statistics or iterative approach where the contribution of features are measured.\n",
    "\n",
    "# ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    "The key principle of feature extraction is to build important features from data by reformatting, combining and transforming features over and over again until a new set of features is created which can give more concise information to a machine learning model than some high dimensional data.\n",
    "\n",
    "The MNIST database consists of 60,000 digits data as training data. Such a high dimensionality data has to be reduced for the machine learning model to be more accurate and take less training time. Dimensionality reduction in the form of feature extraction is done where the features are reduced to a combination of variables that can still describe the same data as before. Non Linear Dimensionality Reduction, Principal Component Analysis method are some of the most widely used techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d8c45",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "\n",
    "Text categorization or text classification based on tags requires a feature engineering because machine cannot understand words.\n",
    "\n",
    "After the corpus has been converted to a numerical data using TF-IDF or Bag of Words, feature engineering technique is applied so that parts of speech tagging is more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d2858",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "\n",
    "Cosine Similarity a measure of similarity and is used for its good performance and computational efficiency. Even if two similar documents are far apart in terms of distance and frequency of words differ, presence of smaller angles between the vectors of data points make it beneficial for detection of similarity and hence cosine similarity is able to check whether the classifications made are correct or not. Duplication of words in text is important from analysis point of view and is good for measuring this duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf2f5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6753032524419089\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "numerator = (2*2)+(3*1)+(2*0)+(0*0)+(2*3)+(3*2)+(3*1)+(0*3)+(1*1)\n",
    "\n",
    "denominator = sqrt(4+9+4+4+9+9+1)*sqrt(4+1+9+4+1+9+1)\n",
    "\n",
    "similarity = numerator/denominator\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1903f14",
   "metadata": {},
   "source": [
    "# Q7.\n",
    "\n",
    "1 . Hamming distance is calculated as the number of bit places where any two bit strings are different. Hamming distance between 10001011 and 11001111 would be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "487dc96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "A = (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "\n",
    "C = (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "Jacc_ind1 = (2/4)*100\n",
    "\n",
    "SMC1 = 5/8\n",
    "\n",
    "print(Jacc_ind1)\n",
    "\n",
    "print(SMC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3cf29f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "B = (1, 1, 0, 0, 0, 1, 1, 1)\n",
    "\n",
    "C = (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "Jacc_ind2 = (2/4)*100\n",
    "\n",
    "SMC2 = 3/8\n",
    "\n",
    "print(Jacc_ind2)\n",
    "\n",
    "print(SMC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c9c22",
   "metadata": {},
   "source": [
    "# Q8.\n",
    "\n",
    "High dimensional data set means that there are high number of features present in the dataset, so high that dimensionality of the dataset becomes detrimental to the learning phase of the machine learning model. In a high dimensional data, number of features can be greater than or equal to the number of observations.\n",
    "\n",
    "Real life examples:\n",
    "\n",
    "Gene sequencing data, spatio-temporal data, sociology based data.\n",
    "\n",
    "The time space complexity is different for each machine learning algorithm, but each suffers from high dimensional data. The curse of dimensionality is at play here. High dimensional data not only slows down the training phase because of the number of computations the algorithm needs to perform increases in quadratic and cubic orders for some algorithms, it also affects the accuracy of algorithms due to presence of correlations that do not make sense and also results in overfitting.\n",
    "\n",
    "High dimensional data can be processed using feature selection procedures, or dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba39d7e",
   "metadata": {},
   "source": [
    "# Q9.\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "2. Use of vectors\n",
    "3. Embedded technique\n",
    "# i. PCA is an acronym for Personal Computer Analysis.\n",
    "PCA is not an acronym for Personal Computer Analysis, but for Principal Component Analysis. Principal Component Analysis is a dimensionality reduction technique used to reduce dimensionality of a dataset. PCA uses calculation of eigen values and eigen vectors to create new features that contribute most to the training phase without affecting training times.\n",
    "\n",
    "# ii. Use of vectors\n",
    "Vectors find high usage in linear algebra and linear algebra based applications like the Support Vector Machines for classification problems. Vectors are often used to understand machine learning algorithms via geometric intuition. Vectors are also used in Exploratory Data Analysis and handling of data.\n",
    "\n",
    "# iii. Embedded technique\n",
    "Embedded technique or method is a one of feature selection approaches in which the feature selection takes place while the model is being trained. Algorithms take care of feature selection on their own and train themselves at the same time. Algorithms like Random Forest support feature importance and can identify which feature is being used most to train the machine learning model.\n",
    "\n",
    "Regularization methods like L1 regularization is also used in feature selection as penalties are imposed and coefficents for features not contributing to the training phase can be put to 0, hence effectively removing the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb392f1c",
   "metadata": {},
   "source": [
    "# Q10.\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "3. SMC vs. Jaccard coefficient\n",
    "# i. Sequential backward exclusion vs. sequential forward selection\n",
    "sequential forward selection:--- Against a defined metric, first feature is chosen after a model is trained and produces the metric. Next feature is selected by searching for that feature which when clubbed with the first chosen feature gives best performance metric results via model training. More and more features are selected one by one while measuring the contribution of the clubbed features in each iteration against the metric using model trained on clubbed features till the required number of features are selected.\n",
    "\n",
    "Sequential backward exclusion:--- A model is trained on all features and the metric score is calculated. In each iteration, a feature is found which if removed from the collection of features, improves the metric score. This goes on and features are discarded one by one, improving performance metric score with each iteration until predefined number of features are left.\n",
    "\n",
    "# ii. Function selection methods: filter vs. wrapper\n",
    "Filter methods rely on finding the statistics for each feature to select the features with highest contributions to the machine learning model. These statistics are calculated for both categorical and numerical data. For numerical data, statistics like Correlation coefficients and for categorical data, statistics like the Chi-Square test are applied between two features to find probability of correlation(linear and non-linear).\n",
    "\n",
    "Wrapper approach is an iterative approach in feature selection where all combinations of subset of features is used to train the model and a perfomance metric is used to test the model and check of there is an improvement of score over the previous iteration where a different subset of features were used. Examples of this technique are forward feature selection and backward feature selection.\n",
    "\n",
    "# iii. SMC vs. Jaccard coefficient\n",
    "SMC or Simple Matching Index is defined as the number of matching attributes divided by total number of attributes. SMC can only be applied to observations with equal number of data points. SMC is most useful in binary attributes.\n",
    "\n",
    "Jaccard coefficient, also called Jaccard index is a measure of similarity of sets. It is defined as Modulus of A intersection B divided by modulus of A union B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297119f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
