{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d49e83",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "\n",
    "In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own. Unsupervised ML Examples 1.A subgroup of cancer patients grouped by their gene expression measurements 2.Groups of shopper based on their browsing and purchasing histories 3.Movie group by the rating given by movies viewers\n",
    "\n",
    "Supervised ML examples 1.Identify whether it is cat or dog 2.House price prediction 3.Predicting weather conditions Hence input data is labeled here whereas in unsupervised the input data is not labeled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc719b",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "\n",
    "1. A subgroup of cancer patients grouped by their gene expression measurements\n",
    "2. Groups of shopper based on their browsing and purchasing histories\n",
    "3. Movie group by the rating given by movies viewers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3703ca0",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "\n",
    "Density-Based Clustering In this method, the clusters are created based upon the density of the data points which are represented in the data space. The regions that become dense due to the huge number of data points residing in that region are considered as clusters.The data points in the sparse region (the region where the data points are very less) are considered as noise or outliers.\n",
    "\n",
    "Examples of DBSCAN (Density-Based Spatial Clustering of Applications with Noise):\n",
    "\n",
    "1. DBSCAN groups data points together based on the distance metric and criterion for a minimum number of data points.\n",
    "2. OPTICS (Ordering Points to Identify Clustering Structure) It is similar in process to DBSCAN, but it attends to one of the drawbacks of the former algorithm i.e. inability to form clusters from data of arbitrary density.\n",
    "3. HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) HDBSCAN is a density-based clustering method that extends the DBSCAN methodology by converting it to a hierarchical clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b0a6c",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "\n",
    "1. K centroids are created randomly (based on the predefined value of K)\n",
    "2. K-means allocates every data point in the dataset to the nearest centroid (minimizing Euclidean distances between them), meaning that a data point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid.\n",
    "3. Then K-means recalculates the centroids by taking the mean of all data points assigned to that centroid’s cluster, hence reducing the total intra-cluster variance in relation to the previous step. The “means” in the K-means refers to averaging the data and finding the new centroid.\n",
    "4. The algorithm iterates between steps 2 and 3 until some criteria is met (e.g. the sum of distances between the data points and their corresponding centroid is minimized, a maximum number of iterations is reached, no changes in centroids value or no data points change clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59822f93",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "\n",
    "A problem with the K-Means clustering is that the final centroids are not interpretable or in other words, centroids are not the actual point but the mean of points present in that cluster.\n",
    "\n",
    "The idea of K-Medoids clustering is to make the final centroids as actual data-points. This result to make the centroids interpretable.\n",
    "\n",
    "Update centroids: In the case of K-Means we were computing mean of all points present in the cluster. But for the PAM algorithm updation of the centroid is different. If there are m-point in a cluster, swap the previous centroid with all other (m-1) points from the cluster and finalize the point as new centroid that have a minimum loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88ca4a",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "\n",
    "It is a diagram that helps to do the visualization for hierarchical clustering. The main objective of using the Dendrogram is to make an optimal number of clusters and to allocate objects to clusters.\n",
    "\n",
    "Consider the closest point and make it a cluster-based upon Euclidean Distance. y-axis is the closest Euclidean Distance and the x-axis is having all data points. Again, find the clusters that are having the least distance between them.\n",
    "\n",
    "Now, p2 and p3 are having its own cluster and p1 is closest to it so we create a full cluster and plot it on the dendrogram graph.\n",
    "\n",
    "Similarly, with P6, P5, and P4\n",
    "\n",
    "Now, combine 2 clusters into a single cluster, and our dendrogram graph is ready.\n",
    "\n",
    "So from the above example, we have created a dendrogram no we need to find what is an optimal number of the cluster from the dendrogram. Let us draw a horizontal line on the dendrogram graph.\n",
    "\n",
    "It means the number of lines cut from the threshold horizontal line that is the number of clusters formed. So 2 clusters are formed from the above example.\n",
    "\n",
    "So, now the important question is when the distance is the largest, we can set that line as the threshold.\n",
    "\n",
    "So, the line cutting the largest distance is a threshold line that is, 2 clusters are formed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de323039",
   "metadata": {},
   "source": [
    "# Q7.\n",
    "\n",
    "SSE is the sum of the squared differences between each observation and its group's mean. It can be used as a measure of variation within a cluster. If all cases within a cluster are identical the SSE would then be equal to 0. The quality of the cluster assignments is determined by computing the sum of the squared error (SSE) after the centroids converge, or match the previous iteration’s assignment. The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is a measure of error, the objective of k-means is to try to minimize this value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6927e11",
   "metadata": {},
   "source": [
    "# Q8.\n",
    "\n",
    "Let X = {x1,x2,x3,……..,xn} be the set of data points and V = {v1,v2,…….,vc} be the set of centers.\n",
    "\n",
    "1) Randomly select ‘c’ cluster centers.\n",
    "\n",
    "2) Calculate the distance between each data point and cluster centers.\n",
    "\n",
    "3) Assign the data point to the cluster center whose distance from the cluster center is minimum of all the cluster centers..\n",
    "\n",
    "4) Re-calculate the new cluster center using: where, ‘ci’ represents the number of data points in ith cluster.\n",
    "\n",
    "5) Recalculate the distance between each data point and new obtained cluster centers.\n",
    "\n",
    "6) If no data point was reassigned then stop, otherwise repeat from step 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d6057",
   "metadata": {},
   "source": [
    "# Q9.\n",
    "\n",
    "In single-link clustering or single-linkage clustering , the similarity of two clusters is the similarity of their most similar members . This single-link merge criterion is local. We pay attention solely to the area where the two clusters come closest to each other. Other, more distant parts of the cluster and the clusters' overall structure are not taken into account.\n",
    "\n",
    "In complete-link clustering or complete-linkage clustering , the similarity of two clusters is the similarity of their most dissimilar members. This is equivalent to choosing the cluster pair whose merge has the smallest diameter. This complete-link merge criterion is non-local; the entire structure of the clustering can influence merge decisions. This results in a preference for compact clusters with small diameters over long, straggly clusters, but also causes sensitivity to outliers. A single document far from the center can increase diameters of candidate merge clusters dramatically and completely change the final clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a4437",
   "metadata": {},
   "source": [
    "# Q10.\n",
    "\n",
    "The Apriori algorithm uses frequent itemsets to generate association rules, and it is designed to work on the databases that contain transactions. With the help of these association rule, it determines how strongly or how weakly two objects are connected. This algorithm uses a breadth-first search and Hash Tree to calculate the itemset associations efficiently. It is the iterative process for finding the frequent itemsets from the large dataset. It is mainly used for market basket analysis and helps to find those products that can be bought together. It can also be used in the healthcare field to find drug reactions for patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd829ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
