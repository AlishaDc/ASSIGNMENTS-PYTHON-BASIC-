{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e5a2e1",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "\n",
    "The depth of a well-balanced binary tree containing m leaves is equal to log2(m), rounded up. A binary Decision Tree (one that makes only binary decisions, as is the case of all trees in Scikit-Learn) will end up more or less well balanced at the end of training, with one leaf per training instance if it is trained without restrictions. Thus, if the training set contains one million instances, the Decision Tree will have a depth of log2 (106) ≈ 20 (actually a bit more since the tree will generally not be perfectly well balanced)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e5dd3",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "\n",
    "A node's Gini impurity is generally lower than its parent's. This is ensured by the CART training algorithm's cost function, which splits each node in a way that minimizes the weighted sum of its children's Gini impurities. However, if one child is smaller than the other, it is possible for it to have a higher Gini impurity than its parent, as long as this increase is more than compensated for by a decrease of the other child's impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf9fb7",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "If a Decision Tree is overfitting the training set, it may be a good idea to decrease max_depth, since this will constrain the model, regularizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8f62b",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "Decision trees and ensemble methods do not require feature scaling to be performed as they are not sensitive to the the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00162b",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "The computational complexity of training a Decision Tree is O(n × m log(m)). So if you multiply the training set size by 10, the training time will be multiplied by K = (n × 10m × log(10m)) / (n × m × log(m)) = 10 × log(10m) / log(m). If m = 10^6, then K ≈ 11.7, so you can expect the training time to be roughly 11.7 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3b8d6",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "\n",
    "Presorting the training set speeds up raining only if the dataset is smaller than a few thousand instances. If it contains 100,000 instances, setting presort=True will considerably slow down training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefaae6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
